{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, datetime, h5py\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense,  Dropout, Activation, Concatenate, Lambda\n",
    "#from tensorflow.python.keras.summary import merge\n",
    "from tensorflow.keras.utils import plot_model, multi_gpu_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from functools import partial, update_wrapper\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(np.random.randint(100000))\n",
    "\n",
    "datadir = \"/home/k1onoda/work/DeepSurvival_20191111\".replace('/', os.sep)\n",
    "\n",
    "isAAL = 0\n",
    "withAge = 0\n",
    "withMMSE = 0\n",
    "permutation = 0\n",
    "\n",
    "onlyMCI = 0;\n",
    "\n",
    "Itr = 100\n",
    "\n",
    "CVs = 10\n",
    "DROPOUT_RATIO = 0.3\n",
    "NB_EPOCH = 1000\n",
    "BATCH_SIZE = 128\n",
    "N_GPUS = 1\n",
    "#MRI = 2 # 1:1.5T 2: 3.0T\n",
    "\n",
    "if  isAAL == 0:\n",
    "    subdir_name = \"BN_Age\" +str(withAge)+\"_MMSE\"+str(withMMSE)\n",
    "else:\n",
    "    subdir_name = \"AAL_Age\"+str(withAge)+\"_MMSE\"+str(withMMSE)\n",
    "\n",
    "if permutation == 1:\n",
    "    subdir_name = subdir_name + \"_permutation\"\n",
    "if onlyMCI == 1:\n",
    "    subdir_name = subdir_name + \"_onlyMCI\"\n",
    "    \n",
    "subdir = datadir + os.sep + subdir_name\n",
    "os.makedirs(subdir, exist_ok = True)  \n",
    "\n",
    "J = 13\n",
    "FINAL1 = np.zeros([Itr,J])\n",
    "\n",
    "for ii in range(Itr):\n",
    "    \n",
    "    print(ii)\n",
    "    K.clear_session()\n",
    "    os.chdir( datadir )\n",
    "    data = io.loadmat(\"GMV_20191111.mat\")\n",
    "    gmv = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "    if isAAL == 1:\n",
    "        data = io.loadmat(\"GMV_AAL_20191111.mat\")\n",
    "        gmv = np.array( data[\"GMV\"], dtype = 'float32' )\n",
    "    nROIs = gmv.shape[1]    \n",
    "    \n",
    "    data = io.loadmat(\"Demo_20191111.mat\" )\n",
    "    demo = np.array( data[\"Demo\"], dtype = 'float32'  )   # database group mri subjectID age sex convert interval MMSE\n",
    "\n",
    "    #mristrength = np.array([demo[:,2]/2]).T\n",
    "    #gmv = np.concatenate([gmv,mristrength],axis=1) \n",
    "    if withAge==1:\n",
    "        age = np.array([demo[:,4]/100]).T\n",
    "        gmv = np.concatenate([gmv, age],axis=1)\n",
    "    if withMMSE==1:\n",
    "        mmse = np.array([demo[:,8]/30]).T\n",
    "        gmv = np.concatenate([gmv, mmse],axis=1)\n",
    "        gmv = np.delete(gmv, np.where(demo[:,8]==0), 0)\n",
    "        demo = np.delete(demo, np.where(demo[:,8]==0), 0)\n",
    "    \n",
    "    if onlyMCI==1:\n",
    "        gmv = np.delete(gmv, np.where(demo[:,1]==3), 0)\n",
    "        demo = np.delete(demo, np.where(demo[:,1]==3), 0)\n",
    "    \n",
    "    n_features = gmv.shape[1]   \n",
    "    nn = gmv.shape[0];\n",
    "    cv = np.zeros([nn])\n",
    "    \n",
    "    converter = (demo[:,6] == 1)  \n",
    "    n_converter = np.sum(converter)\n",
    "    sample_converter = np.arange(n_converter)\n",
    "    np.random.shuffle(sample_converter)\n",
    "    cv_converter = np.array(sample_converter % CVs)\n",
    "    cv[np.where(converter)] = cv_converter\n",
    "    \n",
    "    nonconverter = (demo[:,6] == 0)  \n",
    "    n_nonconverter = np.sum(nonconverter)\n",
    "    sample_nonconverter = np.arange(n_nonconverter)\n",
    "    np.random.shuffle(sample_nonconverter)\n",
    "    cv_nonconverter = np.array(sample_nonconverter % CVs)\n",
    "    cv[np.where(nonconverter)] = cv_nonconverter\n",
    "    \n",
    "    target_train = (cv>1)\n",
    "    target_vali = (cv==0)\n",
    "    target_test = (cv==1)\n",
    "\n",
    "    features_train = gmv[target_train,:]\n",
    "    features_vali = gmv[target_vali,:];\n",
    "    features_test = gmv[target_test,:];\n",
    "    \n",
    "    t = demo[:,7]\n",
    "    e = demo[:,6]\n",
    "    if  permutation == 1:\n",
    "        np.random.shuffle(t)\n",
    "        np.random.shuffle(e)\n",
    "\n",
    "    #J = np.int( np.round( np.max(t) ) + 1 )#J = np.int(np.ceil(np.max(t)) + 1)\n",
    "\n",
    "    T = np.round(t).astype('int64')#T = np.ceil(t).astype('int64')\n",
    "    T[T==0] = 1\n",
    "    E = np.zeros([nn,J])\n",
    "    mask = np.ones([nn,J]).astype('float32')\n",
    "    for num in range(nn):\n",
    "        if e[num] == 1:\n",
    "            E[num,T[num]:J] = 1\n",
    "        if e[num] == 0:\n",
    "            mask[num,T[num]+1:J] = 0\n",
    "    \n",
    "    E_train = E[target_train,:]\n",
    "    E_vali = E[target_vali,:]\n",
    "    E_test = E[target_test,:]\n",
    "    \n",
    "    T_test = T[target_test]\n",
    "    e_test = e[target_test]\n",
    "    \n",
    "    mask_train = mask[target_train,:]\n",
    "    mask_vali = mask[target_vali,:]\n",
    "    mask_test = mask[target_test,:]\n",
    "\n",
    "    todaydetail  =  datetime.datetime.today()\n",
    "    os.chdir(subdir)\n",
    "    logdir = subdir + os.sep + \"log_DeepSurv_\" + todaydetail.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    os.makedirs(logdir, exist_ok = True)\n",
    "    os.chdir(logdir)\n",
    "    print(os.getcwd())\n",
    "    experiment_name = 'deepsurv' \n",
    "\n",
    "    from tensorflow.python.client import device_lib\n",
    "    device_lib.list_local_devices()\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "\n",
    "        def output_of_lambda(input_shape):\n",
    "            shape = list(input_shape)\n",
    "            return (shape[0], J)\n",
    "\n",
    "        def weibull_cdf(parameters):\n",
    "            m = parameters[:,0]\n",
    "            s = tf.maximum( parameters[:,1], 0.001 )\n",
    "            output_list = []\n",
    "            for num in range( J ):\n",
    "                Time   = tf.constant( num, dtype=\"float32\")\n",
    "                e_Time = tf.pow( Time, m )\n",
    "                s_Time = tf.negative( tf.div( e_Time, s) )\n",
    "                x = tf.subtract( tf.constant(1, dtype=\"float32\") , tf.exp( s_Time ) ) # F(t) = 1 - exp(-(t-g)^m/s) #ref http://www.mogami.com/notes/weibull.html\n",
    "                output_list.append ( x )\n",
    "            return tf.stack(output_list, axis=1)\n",
    "\n",
    "        def generator_loss(y_true, y_pred, weights):  # y_true's shape=(batch_size, row, col, ch)\n",
    "            #loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( y_pred, y_true ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "            log_p = tf.log( tf.add( y_pred,  tf.constant(1.0) ) )\n",
    "            log_t = tf.log( tf.add( y_true,  tf.constant(1.0) ) )\n",
    "            loss = tf.cumsum( tf.multiply( tf.square( tf.subtract( log_p, log_t ) ), weights ), axis=1, reverse=True)[:,0]\n",
    "            return loss\n",
    "\n",
    "        def wrapped_generator_loss(func, *args, **kwargs):\n",
    "            partial_generator_loss = partial(generator_loss, *args, **kwargs)\n",
    "            update_wrapper(partial_generator_loss, generator_loss)\n",
    "            return partial_generator_loss\n",
    "\n",
    "        inputs = Input((n_features,), name='inputs')\n",
    "        x1 = Dense(units=32, activation='relu', name='hidden_layer1',\n",
    "                    kernel_regularizer=regularizers.l1_l2(0.001))(inputs)\n",
    "        x1 = Dropout(DROPOUT_RATIO)(x1)\n",
    "        x2 = Dense(units=32, activation='relu', name='hidden_layer2',\n",
    "                    kernel_regularizer=regularizers.l1_l2(0.001))(x1)\n",
    "        x2 = Dropout(DROPOUT_RATIO)(x2)\n",
    "        x3 = Dense(units=32, activation='relu', name='hidden_layer3',\n",
    "                    kernel_regularizer=regularizers.l1_l2(0.001))(x2)\n",
    "        x3 = Dropout(DROPOUT_RATIO)(x3)\n",
    "        p1 = Dense(units=1, activation='softplus', name='param1_layer')(x3)\n",
    "        p2 = Dense(units=1, activation='relu', name='param2_layer')(x3)\n",
    "        parameters = Concatenate(name='params_layer')([p1, p2])\n",
    "        y_pred = Lambda(weibull_cdf, output_shape=output_of_lambda)(parameters)\n",
    "\n",
    "        mask_batch = Input((J,), name='mask_bartch')\n",
    "        L = wrapped_generator_loss(generator_loss, weights = mask_batch)\n",
    "\n",
    "        model = Model(inputs= [inputs, mask_batch], outputs = y_pred)\n",
    "        #model.summary()\n",
    "    \n",
    "    if  N_GPUS>=2:\n",
    "        models = multi_gpu_model(model, gpus=N_GPUS)\n",
    "    else:\n",
    "        models = model\n",
    "\n",
    "    pred_params = np.zeros([nn,2])\n",
    "    c_index_test = np.zeros([J,1])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features_train)\n",
    "    features_train = scaler.transform(features_train)\n",
    "    features_vali = scaler.transform(features_vali)\n",
    "    features_test = scaler.transform(features_test)\n",
    "\n",
    "    outputfilename     = 'Training.csv'\n",
    "    weightfilename     = 'WeightBest.h5'\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=weightfilename, monitor='loss', verbose=1, save_best_only=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',patience=10,verbose=1)\n",
    "    callbacks = []\n",
    "    callbacks.append(early_stopping)\n",
    "    callbacks.append(CSVLogger(outputfilename))\n",
    "    #callbacks.append(checkpointer)\n",
    "\n",
    "    adm = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    models.compile(optimizer=adm, loss=L)\n",
    "    #models.compile(optimizer='Adam', loss='mean_squared_error', metrics=[\"accuracy\"])\n",
    "    session = K.get_session()\n",
    "\n",
    "    models.fit([features_train, mask_train], E_train, batch_size=BATCH_SIZE, epochs = NB_EPOCH, callbacks=callbacks, verbose=0, validation_data = ([features_vali, mask_vali], E_vali))\n",
    "    model.save_weights('Model_Weights_' + str(num+1) + '.h5')\n",
    "\n",
    "    #intermediate_model = Model(inputs=model.input, outputs=model.get_layer('params_layer').output)\n",
    "\n",
    "    #models.load_weights(weightfilename)\n",
    "    #models.compile(optimizer='Adam', loss=L)\n",
    "    prob = models.predict([features_test, mask_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "    intermediate_model = Model(inputs=model.input, outputs=model.get_layer('params_layer').output)\n",
    "    pred_params = intermediate_model.predict([features_test, mask_test])\n",
    "\n",
    "    predictionfilename = 'Param.csv'\n",
    "    prediction = np.c_[demo[target_test,:],pred_params, prob]\n",
    "    np.savetxt(predictionfilename, prediction, delimiter=',')\n",
    "\n",
    "    c_index = np.zeros([J])\n",
    "    for num in range(J - 1) :\n",
    "        c_index[num+1] = concordance_index(np.array(T_test),1/prob[:,num+1], np.array(e_test))\n",
    "    print( c_index )\n",
    "\n",
    "    cindexfilename = 'C_index.txt'\n",
    "    np.savetxt(cindexfilename, c_index)\n",
    "    \n",
    "    FINAL1[ii,:] = c_index.T\n",
    "\n",
    "    #json_string = models.to_json()\n",
    "    #modeltxtfilename   = 'Modeltxt_' + 'Demo.txt'\n",
    "    #f = open(modeltxtfilename,'w')\n",
    "    #f.write(json_string)\n",
    "    #f.close()\n",
    "\n",
    "col_header1 = []\n",
    "for t in range(12):\n",
    "    col_header1.append(str(t+1) + 'yr c_index')\n",
    "    \n",
    "df_ci = pd.DataFrame(FINAL1[:,1:J], columns=col_header1)\n",
    "df_ci.to_csv(subdir + '/result_CINDEX.csv')\n",
    "print(np.mean(np.mean(FINAL1,axis=1),axis=0))    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "121af758ce26dbffc3f548dd04089a35327e4b9202c7124d9bdcd291bcaaa3b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
